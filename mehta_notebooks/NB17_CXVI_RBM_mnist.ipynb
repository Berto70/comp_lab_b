{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 17: Energy-based Generative Models for MNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goal\n",
    "The goal of this notebook is to familiarize readers with various energy-based generative models including: Restricted Boltzmann Machines (RBMs) with Gaussian and Bernoulli units, Deep Boltzmann Machines (DBMs), as well as techniques for training these model including contrastive divergence (CD) and persistent constrastive divergence (PCD). We will also discuss how to generate new examples (commonly called fantasy particles in the ML literature). The notebook also introduces the [Paysage](https://github.com/drckf/paysage) package from [UnLearn.AI](https://unlearn.ai/) for quickly building and training these models..\n",
    "\n",
    "## Overview\n",
    "In this notebook, we study the MNIST dataset using generative models. \n",
    "\n",
    "Let us adopt a different perspective on the MNIST dataset. Generative models, as the name suggests, are useful to generate brand new data (images) by learning how to imitate the ones from a given data set. If you wish, they can be regarded as a kind of \"creative ML\". This is achieved by extracting/learning a set of features which represent the backbone of the entire data and are thus definitive for the particular dataset. These features are encoded in the weights of the model. \n",
    "\n",
    "Mathematically speaking, <mark>generative models are designed and trained to learn an approximation for the probability distribution that generated the data</mark>. Intuitively, this task is much more complex and sophisticated than the image recognition task. Therefore, we shall approach the problem in a number of steps of increasing complexity. For a more detailed discussion on the theory, we invite the reader to check out Secs. XV and XVI of the review.\n",
    "\n",
    "Below, we analyze four different generative models: the Hopfield Model, a Restricted Boltzmann Machine (RBM), a regularized RMB with sparse weights, and a Deep Boltzmann Machine (DBM). In all these cases, we first set up and train the models. After that, we compare the results. Last, we open up the black box of each model and visualise a set of features it learned. \n",
    "\n",
    "## Setting up Paysage\n",
    "\n",
    "In this notebook, we use an open-source `python` package for energy-based models, called [paysage](https://github.com/drckf/paysage). Paysage requires `python>3.5`; we recommend using the package with an [Anaconda](https://www.continuum.io/downloads) environment.\n",
    "\n",
    "To install paysage: \n",
    "\n",
    "* clone or download the [github repo](https://github.com/drckf/paysage)\n",
    "* activate an Anaconda3 environment\n",
    "* navigate to the directory which contains the paysage files\n",
    "* and execute\n",
    "```\n",
    "pip install .\n",
    "```\n",
    "\n",
    "Documentation for paysage is available under [https://github.com/drckf/paysage/tree/master/docs](https://github.com/drckf/paysage/tree/master/docs).\n",
    "\n",
    "By default, computations in paysage are performed using `numpy`/`numexpr`/`numba` on the CPU. If you have installed [PyTorch](https://pytorch.org), then you can switch to the `pytorch` backend by changing the setting in `paysage/backends/config.json` to `pytorch`. \n",
    "\n",
    "Let us set up the required packages for this notebook by importing paysage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install paysage==0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running paysage with the python backend on the cpu\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import paysage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml #MNIST data\n",
    "\n",
    "# for Boltzmann machines\n",
    "from paysage import preprocess as pre\n",
    "from paysage.layers import BernoulliLayer, GaussianLayer\n",
    "from paysage.models import BoltzmannMachine\n",
    "from paysage import batch\n",
    "from paysage import fit\n",
    "from paysage import optimizers\n",
    "from paysage import samplers\n",
    "from paysage import backends as be\n",
    "from paysage import schedules\n",
    "from paysage import penalties as pen\n",
    "\n",
    "# fix random seed to ensure deterministic behavior\n",
    "np.random.seed(137)\n",
    "be.set_seed(137) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the MNIST dataset\n",
    "\n",
    "As we mentioned in the introduction, we use the MNIST dataset of handwritten digits to study the Hopfield model and various variants of RBMs. \n",
    "\n",
    "The MNIST dataset comprises $70000$ handwritten digits, each of which comes in a square image, divided into a $28\\times 28$ pixel grid. Every pixel can take on $256$ nuances of the grey colour, interpolating between white and black, and hence each data point assumes any value in the set $\\{0,1,\\dots,255\\}$. There are $10$ categories in the problem, corresponding to the ten digits. In previous notebooks, we formulated a classification task for the MNIST dataset, and studied it using discriminative supervised learning: Logistic Regression and Deep Neural Networks. \n",
    "\n",
    "\n",
    "This dataset can be fetched using paysage from the web as an `HDF5` file. For this, you should\n",
    "\n",
    "* navigate to the directory in which you cloned/downloaded paysage\n",
    "* navigate further into the `/examples/mnist/` directory which contains the file `download_mnist.py`.\n",
    "* from the terminal, run the command \n",
    "```\n",
    "python3 /examples/mnist/download_mnist.py\n",
    "```\n",
    "\n",
    "This file contains keys `train/images`, `train/labels`, `test/images`, and `test/labels` and is compressed to about 15 Mb in size.\n",
    "\n",
    "As our first step, we will set up the paths to the data and shuffle the dataset. The shuffled dataset will not be compressed (for faster reading during training) so it will be about 56 Mb in size.\n",
    "\n",
    "_Why shuffle the data?_ - Training with stochastic gradient descent means we will be using small minibatches of data (maybe 50 examples) to compute the gradient at each step. If the data have an order, then the estimates for the gradients computed from the minibatches will be biased. Shuffling the data ensures that the gradient estimates are unbiased (though still noisy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path to mnist data:\n",
      "C:\\ProgramData\\Anaconda3\\envs\\complab\\lib\\site-packages\\examples\\mnist\\mnist.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "HDF5 error back trace\n\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5F.c\", line 793, in H5Fopen\n    unable to open file\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5VLcallback.c\", line 3500, in H5VL_file_open\n    open failed\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5VLcallback.c\", line 3465, in H5VL__file_open\n    open failed\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5VLnative_file.c\", line 100, in H5VL__native_file_open\n    unable to open file\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5Fint.c\", line 1707, in H5F_open\n    unable to read superblock\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5Fsuper.c\", line 412, in H5F__super_read\n    file signature not found\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'C:\\ProgramData\\Anaconda3\\envs\\complab\\lib\\site-packages\\examples\\mnist\\mnist.h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHDF5ExtError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\complab\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\complab\\lib\\site-packages\\tables\\file.py\u001b[0m in \u001b[0;36mopen_file\u001b[1;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;31m# Finally, create the File instance, and return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot_uep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\complab\\lib\\site-packages\\tables\\file.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[0;32m    749\u001b[0m         \u001b[1;31m# Now, it is time to initialize the File extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\complab\\lib\\site-packages\\tables\\hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mHDF5ExtError\u001b[0m: HDF5 error back trace\n\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5F.c\", line 793, in H5Fopen\n    unable to open file\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5VLcallback.c\", line 3500, in H5VL_file_open\n    open failed\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5VLcallback.c\", line 3465, in H5VL__file_open\n    open failed\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5VLnative_file.c\", line 100, in H5VL__native_file_open\n    unable to open file\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5Fint.c\", line 1707, in H5F_open\n    unable to read superblock\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5Fsuper.c\", line 412, in H5F__super_read\n    file signature not found\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'C:\\ProgramData\\Anaconda3\\envs\\complab\\lib\\site-packages\\examples\\mnist\\mnist.h5'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8b92c409745c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshuffled_mnist_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataShuffler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffled_mnist_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\complab\\lib\\site-packages\\paysage\\batch\\shuffle.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, shuffled_filename, allowed_mem, complevel, seed)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# get the keys and statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHDFStore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         self.table_stats = {k: TableStatistics(self.store, k)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\complab\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fletcher32\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfletcher32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\complab\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[1;31m# is not part of IOError, make it one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"Unable to open/create file\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: HDF5 error back trace\n\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5F.c\", line 793, in H5Fopen\n    unable to open file\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5VLcallback.c\", line 3500, in H5VL_file_open\n    open failed\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5VLcallback.c\", line 3465, in H5VL__file_open\n    open failed\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5VLnative_file.c\", line 100, in H5VL__native_file_open\n    unable to open file\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5Fint.c\", line 1707, in H5F_open\n    unable to read superblock\n  File \"C:\\ci\\hdf5_1611496732392\\work\\src\\H5Fsuper.c\", line 412, in H5F__super_read\n    file signature not found\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'C:\\ProgramData\\Anaconda3\\envs\\complab\\lib\\site-packages\\examples\\mnist\\mnist.h5'"
     ]
    }
   ],
   "source": [
    "paysage_path = os.path.dirname(os.path.dirname(paysage.__file__))\n",
    "mnist_path = os.path.join(paysage_path, \"examples\", \"mnist\", \"mnist.h5\")\n",
    "shuffled_mnist_path = os.path.join(paysage_path, \"examples\", \"mnist\", \"shuffled_mnist.h5\")\n",
    "\n",
    "print(\"path to mnist data:\")\n",
    "print(mnist_path)\n",
    "\n",
    "#if not os.path.exists(mnist_path):\n",
    "#    raise IOError(\"{} does not exist. run mnist/download_mnist.py to fetch from the web\".format(mnist_path))\n",
    "    \n",
    "if not os.path.exists(shuffled_mnist_path):\n",
    "    batch.DataShuffler(mnist_path, shuffled_mnist_path, complevel=0).shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Data\n",
    "\n",
    "Next, we create a `data` generator, which splits the data into a training and validation sets, and separates them into minibatches of size `batch_size`. Before we begin training, we set `data` into training mode. \n",
    "\n",
    "To monitor the progress of performance metrics during training, we define the variable `performance` which tells Paysage to measure the reconstruction error from the validation set. Possible metrics include the reconstruction error (used in this example) and metrics related to difference in energy of random samples and samples from the model (see [`metrics.md`](https://github.com/drckf/paysage/blob/master/docs/metrics.md) in Paysage documentation for a complete list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### set up minibatch data generator\n",
    "# batch size\n",
    "batch_size=100 \n",
    "transform = pre.Transformation(pre.binarize_color)\n",
    "\n",
    "# create data generator object with minibathces\n",
    "samples = be.float_tensor(pd.read_hdf(shuffled_mnist_path, key='train/images').value)\n",
    "data = batch.in_memory_batch(samples, batch_size, train_fraction=0.95, transform=transform)\n",
    "\n",
    "# reset the data generator in training mode\n",
    "data.reset_generator(mode='train') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Hopfield Model\n",
    "\n",
    "Having loaded and preprocessed the data, we now move on to construct a `hopfield` model. To do this, we use the `Model` class and with a visible `BernoulliLayer` and a hidden `GaussianLayer`. Note that the visible layer has the same size as the input data points, which is can be read off `data.ncols`. The number of hidden units is `num_hidden_units`. We also set the mean and variance of the Gaussian layer to zero and unity, respectively (the notation here is inspired by the terminology in Variational Auto Encoders).\n",
    "\n",
    "We choose to train the model with the `Adam` optimizer. To ensure convergence, we attenuate the `learning_rate` hyperparameter according to a `PowerLawDecay` schedule: `learning_rate`$(t)$ =`initial`$/(1 +$ `coefficient`$\\;\\times\\; t)$. It will prove convenient to define the function `Adam_optimizer()` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### create hopfield model\n",
    "# hidden units\n",
    "num_hidden_units=200 \n",
    "\n",
    "# set up the model\n",
    "vis_layer = BernoulliLayer(data.ncols)\n",
    "hid_layer = GaussianLayer(num_hidden_units)\n",
    "hopfield = BoltzmannMachine([vis_layer, hid_layer])\n",
    "\n",
    "# set mean and standard deviation of hidden layer to to 0 and 1, respectively\n",
    "hopfield.layers[1].set_fixed_params(['loc', 'log_var'])\n",
    "\n",
    "# set up an optimizer method (ADAM in this case)\n",
    "def ADAM_optimizer(initial,coefficient):\n",
    "\t# define learning rate attenuation schedule\n",
    "\tlearning_rate=schedules.PowerLawDecay(initial=initial,coefficient=coefficient)\n",
    "\t# return optimizer object\n",
    "\treturn optimizers.ADAM(stepsize=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling and Training a Model in Paysage\n",
    "\n",
    "Next, we have compile the model. First, we initialize a `model` using the `initialize` function attribute which accepts the `data` as a required argument. We choose the initialization routine `glorot`, see discussion in review. Second, we define an optimizer calling the function `Adam_optimizer()` defined above, and store the object under the name `opt`. To define a Monte Carlo `sampler`, we use the method `from_batch` of the `SequentialMC` class, parsing the `model` and the `data`. Last, we create an `SGD` object called `trainer` to train the model using Persistent Contrastive Divergence (`pcd`) with a fixed number of `monte_carlo_steps`. We can also `monitor` the reconstruction error during training. Last, we train the model in epochs (see variable `nim_epochs`), calling the `train()` method of `trainer`. These steps are universal for shallow generative `model`'s, and it is convenient to combine them in the function `train_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to compile and train model\n",
    "num_epochs=20 # training epochs\n",
    "monte_carlo_steps=1 # number of MC sampling steps\n",
    "def train_model(model,num_epochs,monte_carlo_steps):\n",
    "        # make a simple guess for the initial parameters of the model\n",
    "        model.initialize(data,method='glorot_normal')\n",
    "        # set optimizer\n",
    "        opt=ADAM_optimizer(1E-2,1.0)\n",
    "        trainer = fit.SGD(model, data)\n",
    "        trainer.train(opt, num_epochs, method=fit.pcd, mcsteps=monte_carlo_steps)\n",
    "# train hopfield model\n",
    "train_model(hopfield,num_epochs,monte_carlo_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Generative Models\n",
    "\n",
    "We can easily create a Bernoulli RBM and train it using the functions defined above as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Bernoulli RBM\n",
    "vis_layer = BernoulliLayer(data.ncols)\n",
    "hid_layer = BernoulliLayer(num_hidden_units)\n",
    "rbm = BoltzmannMachine([vis_layer, hid_layer])\n",
    "\n",
    "# train Bernoulli RBM\n",
    "train_model(rbm,num_epochs,monte_carlo_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing a Bernoulli RBM with L1 regularization is also straightforward in Paysage, using the `add_penalty` method which accepts a dictionary as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Bernoulli RBM with L1 regularizer\n",
    "vis_layer = BernoulliLayer(data.ncols)\n",
    "hid_layer = BernoulliLayer(num_hidden_units)\n",
    "rbm_L1 = BoltzmannMachine([vis_layer, hid_layer])\n",
    "\n",
    "rbm_L1.connections[0].weights.add_penalty({'matrix': pen.l1_penalty(1e-3)})\n",
    "\n",
    "# train Bernoulli RBM with L1 regularizer\n",
    "train_model(rbm_L1,num_epochs,monte_carlo_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a deep Boltzmann machine (DBM), we just add more layers, and an L1 penalty for every layer.\n",
    "\n",
    "Recalling the essential trick with layer-wise pre-training to prepare the weights of the DBM, we define a `pretrainer` as an object of the `LayerwisePretrain` class (see code snippet below). This results in a slight modification of the function `train_model`, which we call `train_deep_model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Deep Boltzmann Machine\n",
    "# set up the model\n",
    "dbm = BoltzmannMachine([BernoulliLayer(data.ncols), # visible layer\n",
    "                        BernoulliLayer(num_hidden_units), # hidden layer 1\n",
    "                        BernoulliLayer(num_hidden_units) # hidden layer 2\n",
    "                       ])\n",
    "\n",
    "# add an L1 penalty to the weights\n",
    "for conn in dbm.connections:\n",
    "    conn.weights.add_penalty({'matrix':pen.l1_penalty(1e-3)})\n",
    "    \n",
    "# add pre-training \t\n",
    "def train_deep_model(model,num_epochs,monte_carlo_steps):\n",
    "    # make a simple guess for the initial parameters of the model\n",
    "    model.initialize(data,method='glorot_normal')\n",
    "    # set SGD rÏ€etrain optimizer\n",
    "    opt=ADAM_optimizer(1E-2,1.0)\n",
    "    # pre-train model\n",
    "    pretrainer=fit.LayerwisePretrain(model,data)\n",
    "    pretrainer.train(opt, num_epochs, method=fit.pcd, mcsteps=monte_carlo_steps, init_method=\"glorot_normal\")\n",
    "    # set SGD train optimizer\n",
    "    opt=ADAM_optimizer(1E-3,1.0)\n",
    "    # train model\n",
    "    trainer=fit.SGD(model,data)\n",
    "    trainer.train(opt,num_epochs,method=fit.pcd,mcsteps=monte_carlo_steps)\n",
    "# train DBM\n",
    "train_deep_model(dbm,num_epochs,monte_carlo_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the MNIST Dataset\n",
    "\n",
    "Let us look at a couple of random examples to get an idea how the data we are dealing with actually looks like.\n",
    "\n",
    "To do this, we define the function  `plot_image_grid()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "# make sure the plots are shown in the notebook\n",
    "\n",
    "def plot_image_grid(image_array, shape, vmin=0, vmax=1, cmap=cm.gray_r, row_titles=None):\n",
    "    array = be.to_numpy_array(image_array)\n",
    "    nrows, ncols = array.shape[:-1]\n",
    "    f = plt.figure(figsize=(2*ncols, 2*nrows))\n",
    "    grid = gs.GridSpec(nrows, ncols)\n",
    "    axes = [[plt.subplot(grid[i,j]) for j in range(ncols)] for i in range(nrows)]\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            sns.heatmap(np.reshape(array[i][j], shape),\n",
    "                ax=axes[i][j], cmap=cmap, cbar=False, vmin=vmin, vmax=vmax)\n",
    "            axes[i][j].set(yticks=[])\n",
    "            axes[i][j].set(xticks=[])\n",
    "\n",
    "    if row_titles is not None:\n",
    "        for i in range(nrows):\n",
    "            axes[i][0].set_ylabel(row_titles[i], fontsize=36)\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.show(f)\n",
    "    plt.close(f)\n",
    "    \n",
    "\n",
    "image_shape = (28, 28) # 28x28 = 784 pixels in every image\n",
    "num_to_plot = 8 # of data points to plot\n",
    "\n",
    "examples = data.get(mode='train') # shape (batch_size, 784)\n",
    "\n",
    "example_plot = plot_image_grid(np.expand_dims(examples[:num_to_plot], 0), \n",
    "                               image_shape, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructions\n",
    "\n",
    "Having trained our models, let us see how they perform by computing some reconstructions from the validation data. \n",
    "\n",
    "Recall that a reconstruction ${\\bf v'}$ of a given data point ${\\bf x}$ is computed in two steps: (i) we fix the visible layer ${\\bf v}={\\bf x}$ to be the data, and use MC sampling to find the state of the hidden layer ${\\bf h}$ which maximizes the probability distribution $p({\\bf h}\\vert{\\bf v})$, (ii) fixing the same obtained state ${\\bf h}$, we find the reconstruction of the visible layer ${\\bf v'}$ which maximizes the probability $p({\\bf v'}\\vert{\\bf h})$. In the case of a DBM, the forward pass continues until we reach the last of the hidden layers, and the backward pass goes in reverse. \n",
    "\n",
    "To compute reconstructions, we define a MC `sampler` based on the trained `model`. The stating point form the MC sampler is set using the `set_state()` method. To compute reconstructions, we need to keep the probability distribution encoded in the `model` fixed which is done with the help of the `deterministic_iteration` function method, which takes the number of weights `num_weights` in the model, and the state of the sampler `sampler.state` as required arguments. We can combine these steps in the function `compute_reconstructions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### compute reconstructions\n",
    "def compute_reconstructions(model, data):\n",
    "    \"\"\"\n",
    "    Computes reconstructions of the input data.\n",
    "    Input v -> h -> v' (one pass up one pass down)\n",
    "    \n",
    "    Args:\n",
    "        model: a model\n",
    "        data: a tensor of shape (num_samples, num_visible_units)\n",
    "\n",
    "    Returns:\n",
    "        tensor of shape (num_samples, num_visible_units)\n",
    "    \n",
    "    \"\"\"\n",
    "    recons = model.compute_reconstructions(data).get_visible()\n",
    "    return be.to_numpy_array(recons)\n",
    "\n",
    "examples = data.get(mode='validate') # shape (batch_size, 784)\n",
    "data.reset_generator(mode='validate') # reset the generator to the beginning of the validation set\n",
    "\n",
    "hopfield_reconstructions = compute_reconstructions(hopfield, examples[:num_to_plot])\n",
    "rbm_reconstructions = compute_reconstructions(rbm, examples[:num_to_plot])\n",
    "rbm_L1_reconstructions = compute_reconstructions(rbm_L1, examples[:num_to_plot])\n",
    "dbm_reconstructions = compute_reconstructions(dbm, examples[:num_to_plot])\n",
    "\n",
    "reconstruction_plot = plot_image_grid(\n",
    "    np.array([examples[:num_to_plot], \n",
    "                 hopfield_reconstructions, \n",
    "                 rbm_reconstructions, \n",
    "                 rbm_L1_reconstructions,\n",
    "                 dbm_reconstructions]), \n",
    "    image_shape, vmin=0, vmax=1, row_titles=[\"Data\", \"Hopfield\", \"RBM\", \"RBM (L1)\", \"DBM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fantasy Particles\n",
    "\n",
    "Once we have the trained models ready, we can use MC to draw samples from the corresponding probability distributions, called \"fantasy particles\". To this end let us draw a `random_sample` from the validation data, and compute the `model_state`. Next, we define a MC `sampler` based on the `model`, and set its state to `model_state`. To compute the fantasy particles, we do layer-wise Gibbs sampling for a total of `n_steps` equilibration steps. The last step (controlled by the boolean `mean_field`) is a final mean-field iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fantasy_particles(model,num_fantasy,num_steps,mean_field=True):\n",
    "    \"\"\"\n",
    "    Draws samples from the model using Gibbs sampling Markov Chain Monte Carlo .\n",
    "    Starts from randomly initialized points. \n",
    "\n",
    "    Args:\n",
    "        model: a model\n",
    "        data: a tensor of shape (num_samples, num_visible_units)\n",
    "        num_steps (int): the number of update steps\n",
    "        mean_field (bool; optional): run a final mean field step to compute probabilities\n",
    "\n",
    "    Returns:\n",
    "        tensor of shape (num_samples, num_visible_units)\n",
    "    \n",
    "    \"\"\"\n",
    "    schedule = schedules.Linear(initial=1.0, delta = 1 / (num_steps-1))\n",
    "    fantasy = samplers.SequentialMC.generate_fantasy_state(model,\n",
    "                                                           num_fantasy,\n",
    "                                                           num_steps,\n",
    "                                                           schedule=schedule,\n",
    "                                                           beta_std=0.0,\n",
    "                                                           beta_momentum=0.0)\n",
    "    if mean_field:\n",
    "        fantasy = model.mean_field_iteration(1, fantasy)\n",
    "    fantasy_particles = fantasy.get_visible()        \n",
    "    return be.to_numpy_array(fantasy_particles)\n",
    "\n",
    "examples = data.get(mode='validate') # shape (batch_size, 784)\n",
    "data.reset_generator(mode='validate') # reset the generator to the beginning of the validation set\n",
    "\n",
    "hopfield_fantasy = compute_fantasy_particles(hopfield, num_to_plot, 100, mean_field=False)\n",
    "rbm_fantasy = compute_fantasy_particles(rbm, num_to_plot, 100, mean_field=False)\n",
    "rbm_L1_fantasy = compute_fantasy_particles(rbm_L1, num_to_plot, 100, mean_field=False)\n",
    "dbm_fantasy = compute_fantasy_particles(dbm, num_to_plot, 100, mean_field=False)\n",
    "\n",
    "fantasy_plot = plot_image_grid(\n",
    "    np.array([hopfield_fantasy, \n",
    "                 rbm_fantasy, \n",
    "                 rbm_L1_fantasy,\n",
    "                 dbm_fantasy]), \n",
    "    image_shape, vmin=0, vmax=1, row_titles=[\"Hopfield\", \"RBM\", \"RBM (L1)\", \"DBM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-noising Images\n",
    "\n",
    "One can use generative models to reduce the noise in images (de-noising). Let us randomly flip a fraction, `fraction_to_flip`, of the black & white bits in the validation data, and use the models defined above to reconstruct (de-noise) the digit images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### denoise MNIST images\n",
    "# get validation data\n",
    "examples = data.get(mode='validate') # shape (batch_size, 784)\n",
    "# reset data generator to beginning of the validation set\n",
    "data.reset_generator(mode='validate') \n",
    "\n",
    "# add some noise to the examples by randomly flipping some pixels 0 -> 1 and 1 -> 0\n",
    "fraction_to_flip=0.15\n",
    "# create flipping mask\n",
    "flip_mask=be.rand_like(examples) < fraction_to_flip\n",
    "# compute noisy data\n",
    "noisy_data=(1-flip_mask) * examples + flip_mask * (1 - examples)\n",
    "\n",
    "# define number of digits to display\n",
    "num_to_display=8\n",
    "# compute de-noised images\n",
    "hopfield_denoised=compute_reconstructions(hopfield,noisy_data[:num_to_display])\n",
    "rbm_denoised=compute_reconstructions(rbm,noisy_data[:num_to_display])\n",
    "rbm_L1_denoised=compute_reconstructions(rbm_L1,noisy_data[:num_to_display])\n",
    "dbm_denoised=compute_reconstructions(dbm,noisy_data[:num_to_display])\n",
    "\n",
    "denoising_plot = plot_image_grid(\n",
    "    np.array([examples[:num_to_plot], \n",
    "                 noisy_data[:num_to_plot], \n",
    "                 hopfield_denoised, \n",
    "                 rbm_denoised, \n",
    "                 rbm_L1_denoised,\n",
    "                 dbm_denoised]), \n",
    "    image_shape, vmin=0, vmax=1, row_titles=[\"Data\", \"Noisy\", \"Hopfield\", \"RBM\", \"RBM (L1)\", \"DBM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Visualization\n",
    "\n",
    "Let us open up the black box of our generative models now. Below, we show the features learned by the weights of the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights of the hopfield model\n",
    "hopfield_weights = plot_image_grid(\n",
    "    be.reshape(hopfield.connections[0].weights.W(trans=True)[:25], (5,5,784)), \n",
    "    image_shape, \n",
    "    vmin=be.tmin(hopfield.connections[0].weights.W()), \n",
    "    vmax=be.tmax(hopfield.connections[0].weights.W()),\n",
    "    cmap=cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights of the RBM\n",
    "rbm_weights = plot_image_grid(\n",
    "    be.reshape(rbm.connections[0].weights.W(trans=True)[:25], (5,5,784)), \n",
    "    image_shape, \n",
    "    vmin=be.tmin(rbm.connections[0].weights.W()), \n",
    "    vmax=be.tmax(rbm.connections[0].weights.W()),\n",
    "    cmap=cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights of the L1 regularized RBM\n",
    "rbmL1_weights = plot_image_grid(\n",
    "    be.reshape(rbm_L1.connections[0].weights.W(trans=True)[:25], (5,5,784)), \n",
    "    image_shape, \n",
    "    vmin=be.tmin(rbm_L1.connections[0].weights.W()), \n",
    "    vmax=be.tmax(rbm_L1.connections[0].weights.W()),\n",
    "    cmap=cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights of the first layer of the dbm\n",
    "dbm_weights = plot_image_grid(\n",
    "    be.reshape(dbm.connections[0].weights.W(trans=True)[:25], (5,5,784)), \n",
    "    image_shape, \n",
    "    vmin=be.tmin(dbm.connections[0].weights.W()), \n",
    "    vmax=be.tmax(dbm.connections[0].weights.W()),\n",
    "    cmap=cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.close() # close the HDF5 store with the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Try increasing/decreasing the number of hidden units and study systematically how the performance of the different models changes.\n",
    "* Look up Paysage's documentation and study the performance for various SGD optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
